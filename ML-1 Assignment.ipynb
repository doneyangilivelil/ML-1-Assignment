{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the three stages to build the hypotheses or model in machine learning?\n",
    "2. What is the standard approach to supervised learning?\n",
    "3. What is Training set and Test set?\n",
    "4. What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?\n",
    "5. How can you avoid overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Answer\n",
    "Select Data\n",
    "Preprocess Data\n",
    "Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Answer\n",
    "1. Classification. When the data are being used to predict a category, supervised learning is also called classification.\n",
    "This is the case when assigning an image as a picture of either a 'cat' or a 'dog'. When there are only two choices, \n",
    "it's called two-class or binomial classification. When there are more categories, as when predicting the winner of the\n",
    "NCAA March Madness tournament, this problem is known as multi-class classification.\n",
    "\n",
    "2. Regression. When a value is being predicted, as with stock prices, supervised learning is called regression.\n",
    "\n",
    "3. Anomaly detection. Sometimes the goal is to identify data points that are simply unusual. In fraud detection, \n",
    "for example, any highly unusual credit card spending patterns are suspect. The possible variations are so numerous\n",
    "and the training examples so few, that it's not feasible to learn what fraudulent activity looks like. \n",
    "The approach that anomaly detection takes is to simply learn what normal activity looks like \n",
    "(using a history non-fraudulent transactions) and identify anything that is significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Answer\n",
    "A training dataset is a dataset used to train a model.\n",
    "Specific features are picked out from the training set.These features are then incorporated into the model. \n",
    "The training set is labeled correctly, the model should be able to learn something from these features.\n",
    "\n",
    "The test dataset is a dataset used to measure how well the model performs at making predictions on that test set.\n",
    "If the prediction scores for the test set are unreasonable, we’ll need to make some adjustments to our model and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Answer\n",
    "\n",
    "Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in \n",
    "order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).\n",
    "\n",
    "\n",
    "Ensemble methods can be divided into two groups:\n",
    "\n",
    "1. sequential ensemble methods where the base learners are generated sequentially (e.g. AdaBoost).\n",
    "The basic motivation of sequential methods is to exploit the dependence between the base learners. The overall performance can be boosted by weighing previously mislabeled examples with higher weight.\n",
    "2. parallel ensemble methods where the base learners are generated in parallel (e.g. Random Forest). \n",
    "The basic motivation of parallel methods is to exploit independence between the base learners since the error can be reduced dramatically by averaging.\n",
    "\n",
    "Most ensemble methods use a single base learning algorithm to produce homogeneous base learners, i.e. learners of \n",
    "the same type, leading to homogeneous ensembles.\n",
    "\n",
    "Bagging - stands for bootstrap aggregation. One way to reduce the variance of an estimate is to average together\n",
    "multiple estimates. For example, we can train M different trees on different subsets of the data \n",
    "(chosen randomly with replacement) and compute the ensemble:\n",
    "    \n",
    "Boosting - refers to a family of algorithms that are able to convert weak learners to strong learners. \n",
    "The main principle of boosting is to fit a sequence of weak learners− models that are only slightly better \n",
    "than random guessing, such as small decision trees− to weighted versions of the data. More weight is given \n",
    "\n",
    "to examples that were misclassified by earlier rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Answer\n",
    "Follow below techniques to avoid noise in the traning dataset\n",
    "Cross-validation\n",
    "Train with more data\n",
    "Remove closely correlated features\n",
    "Regularization\n",
    "Ensembling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
